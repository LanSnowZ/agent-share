from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List

import numpy as np

from .config import Config
from .embedding import Embedder
from .llm_qc import LLMQC
from .models import MemoryItem, UserProfile
from .storage import JsonStore
from .utils import l2_normalize


@dataclass
class Peer:
    """Peer class for compatibility, but not used in hybrid retrieval strategy."""
    user_id: str
    profile_text: str


class RetrievePipeline:
    def __init__(self, cfg: Config) -> None:
        self.cfg = cfg
        self.store = JsonStore(cfg)
        self.embed = Embedder(cfg)
        self.llm = LLMQC(cfg)

    def get_cached_peers(self) -> List[Peer]:
        """Returns empty list for compatibility with existing code."""
        return []

    def _encode_query(self, profile_text: str, task: str) -> np.ndarray:
        """
        Encode query by separately encoding profile and task, then fusing them.
        This prevents long profiles from diluting short task queries.
        """
        # If no task is provided, use profile only
        if not task or task.strip() == "":
            text = profile_text or ""
            vec = np.array(self.embed.embed_text(text), dtype=np.float64)
            return l2_normalize(vec)

        # Separately encode profile and task
        profile_vec = np.array(
            self.embed.embed_text(profile_text or ""), dtype=np.float64
        )
        task_vec = np.array(self.embed.embed_text(task), dtype=np.float64)

        # Normalize both vectors
        profile_vec = l2_normalize(profile_vec)
        task_vec = l2_normalize(task_vec)

        # Weighted fusion: use configurable weights
        task_weight = self.cfg.task_weight
        profile_weight = self.cfg.profile_weight

        # Ensure weights sum to 1.0 for proper normalization
        total_weight = task_weight + profile_weight
        if total_weight > 0:
            task_weight = task_weight / total_weight
            profile_weight = profile_weight / total_weight
        else:
            task_weight, profile_weight = 0.7, 0.3  # fallback

        fused_vec = task_weight * task_vec + profile_weight * profile_vec
        return l2_normalize(fused_vec)

    def _hybrid_retrieve_with_focus_and_cot(
        self, memories: List[MemoryItem], query_vec: np.ndarray, top_k: int
    ) -> List[MemoryItem]:
        """
        ç»¼åˆæ£€ç´¢ï¼šåŸºäº focus_query å’Œ COT çš„åŠ æƒç›¸ä¼¼åº¦å¬å›å€™é€‰è®°å¿†
        
        Args:
            memories: æ‰€æœ‰è®°å¿†é¡¹
            query_vec: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›çš„top-Kæ•°é‡
            
        Returns:
            å¬å›çš„è®°å¿†åˆ—è¡¨ï¼ˆæŒ‰ç»¼åˆåˆ†æ•°æ’åºï¼‰
        """
        if not memories or top_k <= 0:
            return []

        # 1. è®¡ç®— focus_query ç›¸ä¼¼åº¦
        focus_queries = [m.focus_query for m in memories]
        focus_vectors = np.array(self.embed.embed_many(focus_queries), dtype=np.float64)
        # L2 normalize each row (each vector)
        focus_vectors = focus_vectors / (np.linalg.norm(focus_vectors, axis=1, keepdims=True) + 1e-12)
        focus_similarities = focus_vectors @ query_vec

        # 2. è®¡ç®— COT ç›¸ä¼¼åº¦
        cot_texts = [m.cot_text for m in memories]
        cot_vectors = np.array(self.embed.embed_many(cot_texts), dtype=np.float64)
        # L2 normalize each row (each vector)
        cot_vectors = cot_vectors / (np.linalg.norm(cot_vectors, axis=1, keepdims=True) + 1e-12)
        cot_similarities = cot_vectors @ query_vec

        # 3. åŠ æƒèåˆ
        # å½’ä¸€åŒ–æƒé‡
        focus_weight = self.cfg.focus_query_weight
        cot_weight = self.cfg.cot_weight
        total_weight = focus_weight + cot_weight
        if total_weight > 0:
            focus_weight = focus_weight / total_weight
            cot_weight = cot_weight / total_weight
        
        hybrid_scores = focus_weight * focus_similarities + cot_weight * cot_similarities

        # 4. æ’åºå¹¶å– top-K
        num_memories = len(memories)
        k = min(top_k, num_memories)
        top_k_indices = np.argsort(-hybrid_scores)[:k]

        # 5. æ—¥å¿—è¾“å‡ºå‰10ä¸ª
        print("\n" + "="*80)
        print("ğŸ“Š [ç²—å¬å›é˜¶æ®µ] ç»¼åˆæ£€ç´¢ç»“æœ (focus_query + COT)")
        print(f"   æ€»è®°å¿†æ•°: {num_memories}, å¬å›æ•°: {k}")
        print(f"   æƒé‡é…ç½®: focus_query={focus_weight:.2f}, COT={cot_weight:.2f}")
        print("-"*80)
        display_count = min(10, len(top_k_indices))
        for rank, idx in enumerate(top_k_indices[:display_count], start=1):
            memory = memories[idx]
            hybrid_score = float(hybrid_scores[idx])
            focus_score = float(focus_similarities[idx])
            cot_score = float(cot_similarities[idx])
            focus_preview = memory.focus_query[:50].replace("\n", " ") if memory.focus_query else "æ— "
            print(f"  {rank:2d}. ID: {memory.id:20s} | ç»¼åˆåˆ†æ•°: {hybrid_score:.4f}")
            print(f"       â””â”€ Focusåˆ†æ•°: {focus_score:.4f} | COTåˆ†æ•°: {cot_score:.4f}")
            print(f"       â””â”€ Focusé¢„è§ˆ: {focus_preview}")
        print("="*80 + "\n")

        return [memories[i] for i in top_k_indices]

    def retrieve(
        self, user: UserProfile, task: str, peers: List[Peer], top_k: int = 5
    ) -> Dict[str, any]:
        """
        ä¸¤æ­¥æ£€ç´¢ç­–ç•¥ï¼š
        1. ç»¼åˆæ£€ç´¢ï¼šåŸºäº focus_query + COT çš„åŠ æƒç›¸ä¼¼åº¦å¬å›å€™é€‰
        2. LLMåˆ¤æ–­ï¼šä½¿ç”¨LLMè¿‡æ»¤å‡ºçœŸæ­£æœ‰ç”¨çš„è®°å¿†
        
        Args:
            user: ç”¨æˆ·æ¡£æ¡ˆ
            task: å½“å‰ä»»åŠ¡/æŸ¥è¯¢
            peers: åŒä¼´åˆ—è¡¨ï¼ˆä¸ºäº†æ¥å£å…¼å®¹ä¿ç•™ï¼Œä½†ä¸ä½¿ç”¨ï¼‰
            top_k: æœ€ç»ˆè¿”å›çš„è®°å¿†æ•°é‡
            
        Returns:
            åŒ…å«æ£€ç´¢ç»“æœçš„å­—å…¸ï¼Œæ ¼å¼ä¿æŒå…¼å®¹
        """
        memories = self.store.list_memories()
        Q_i = self._encode_query(user.profile_text, task)

        # ============ ç¬¬ä¸€æ­¥ï¼šç»¼åˆæ£€ç´¢ï¼ˆfocus_query + COTï¼‰ ============
        recall_k = self.cfg.hybrid_recall_k
        candidate_memories = self._hybrid_retrieve_with_focus_and_cot(
            memories, Q_i, recall_k
        )

        # å¦‚æœæ²¡æœ‰å€™é€‰ï¼Œç›´æ¥è¿”å›ç©º
        if not candidate_memories:
            return {"items": [], "lambda": 0.0, "alpha": [], "peer": []}

        # ============ ç¬¬äºŒæ­¥ï¼šLLM æœ‰ç”¨æ€§åˆ¤æ–­ ============
        # ä»å€™é€‰ä¸­é€‰æ‹©å‰ top_k*2 ä¸ªé€å…¥LLMåˆ¤æ–­ï¼ˆé¿å…è¿‡æ»¤åæ•°é‡ä¸è¶³ï¼‰
        llm_input_k = min(len(candidate_memories), top_k * 2)
        llm_candidates = candidate_memories[:llm_input_k]
        
        # æå– focus_queries ç”¨äºLLMåˆ¤æ–­
        focus_queries_for_llm = [m.focus_query for m in llm_candidates]
        
        # æ—¥å¿—ï¼šé€å…¥LLMçš„è®°å¿†ID
        print("\n" + "="*80)
        print(f"ğŸ¤– [LLMåˆ¤æ–­é˜¶æ®µ] é€å…¥ {llm_input_k} ä¸ªå€™é€‰è®°å¿†è¿›è¡Œæœ‰ç”¨æ€§åˆ¤æ–­")
        print("-"*80)
        try:
            ids_for_llm = [m.id for m in llm_candidates]
            for i, mem_id in enumerate(ids_for_llm, 1):
                print(f"  {i:2d}. ID: {mem_id:20s}")
        except Exception:
            pass
        print("="*80 + "\n")
        
        # è°ƒç”¨LLMæ‰¹é‡åˆ¤æ–­
        useful_flags = self.llm.are_focus_queries_useful(task, focus_queries_for_llm)

        # è¿‡æ»¤å‡ºæœ‰ç”¨çš„è®°å¿†
        filtered_memories = []
        for i, (mem, is_useful) in enumerate(zip(llm_candidates, useful_flags)):
            if is_useful:
                filtered_memories.append(mem)

        # æ—¥å¿—ï¼šLLMåˆ¤æ–­ç»“æœ
        print("\n" + "="*80)
        print(f"âœ… [LLMåˆ¤æ–­ç»“æœ] {len(filtered_memories)}/{llm_input_k} ä¸ªè®°å¿†è¢«åˆ¤å®šä¸ºæœ‰ç”¨")
        print("-"*80)
        for i, mem in enumerate(filtered_memories[:10], 1):
            focus_preview = mem.focus_query[:50].replace("\n", " ") if mem.focus_query else "æ— "
            print(f"  {i:2d}. ID: {mem.id:20s}")
            print(f"       â””â”€ Focus: {focus_preview}")
        print("="*80 + "\n")

        # è¿”å›æœ€ç»ˆçš„ top_k ç»“æœ
        final_k = min(top_k, len(filtered_memories))
        results = [
            {
                "rank": int(r + 1),
                "score": 1.0 - (r / max(len(filtered_memories), 1)),  # ç®€å•çš„é€’å‡åˆ†æ•°
                "memory": filtered_memories[r].to_dict(),
            }
            for r in range(final_k)
        ]
        
        # è¿”å›æ ¼å¼ä¿æŒå…¼å®¹ï¼Œä½†lambda/alpha/peeråœ¨æ–°ç­–ç•¥ä¸­ä¸ä½¿ç”¨
        return {
            "items": results,
            "lambda": 0.0,  # æ–°ç­–ç•¥ä¸ä½¿ç”¨lambda
            "alpha": [],    # æ–°ç­–ç•¥ä¸ä½¿ç”¨alpha
            "peer": [],     # æ–°ç­–ç•¥ä¸ä½¿ç”¨peer
        }

    def build_prompt_blocks(
        self,
        items: List[Dict[str, any]],
        conversation_id: str = None,
        username: str = None,
    ) -> str:
        parts: List[str] = []
        selected_ids: List[str] = []
        for i, it in enumerate(items, start=1):
            mem = it["memory"]
            cot = mem.get("cot_text", "")
            focus_query = mem.get("focus_query", "")
            source_user_id = mem.get("source_user_id", "")
            kg = mem.get("meta", {}).get("kg", [])
            # print(cot)
            # print(kg)
            print("##############################################")
            print(f"ğŸ” build_prompt_blocks - Memory #{i}")
            print(f"  - ID: {mem.get('id', 'NO_ID_FOUND')}")
            print(f"  - Type of mem: {type(mem)}")
            print(f"  - Keys in mem: {list(mem.keys())}")
            print("##############################################")
            # Record selected memory id for final summary print
            memory_id = mem.get("id", "NO_ID_FOUND")
            selected_ids.append(memory_id)
            print(f"  - æ·»åŠ è®°å¿†IDåˆ°selected_ids: {memory_id}")
            # Get the static profile of the memory creator
            creator_profile = ""
            if source_user_id:
                creator_user = self.store.get_user(source_user_id)
                if creator_user:
                    creator_profile = creator_user.profile_text

            # Build the memory block with focus_query and creator profile
            parts.append(f"### Memory #{i}")
            if focus_query:
                parts.append(f"**Focus Query:** {focus_query}")
            # if creator_profile:
            #     parts.append(f"**Created by:** {creator_profile}")
            # parts.append(f"**Content:** {cot}")
            parts.append("**KG:**")
            for e in kg:
                head = e.get("head", "?")
                rel = e.get("relation", "rel")
                tail = e.get("tail", "?")
                parts.append(f"- ({head}, {rel}, {tail})")
            parts.append("")  # Add empty line between memories
            with open("local/cotkg.txt", "a+") as f:
                f.write(cot)
                f.write("\n\n--- Knowledge Graph ---\n")
                for e in kg:
                    head = e.get("head", "?")
                    rel = e.get("relation", "rel")
                    tail = e.get("tail", "?")
                    f.write(f"- ({head}, {rel}, {tail})\n")
        # Final concise log of memory IDs added to the prompt
        if selected_ids:
            try:
                print(f"âœ… æœ€ç»ˆåŠ å…¥æç¤ºè¯çš„å…±äº«è®°å¿†ID: {', '.join(selected_ids)}")
            except Exception:
                # Fallback to avoid any unexpected printing errors
                print("âœ… æœ€ç»ˆåŠ å…¥æç¤ºè¯çš„å…±äº«è®°å¿†ID:", selected_ids)

        # è®°å¿†IDçš„ä¿å­˜ç°åœ¨é€šè¿‡save_chat_conversationå‡½æ•°å®Œæˆ
        print("\nğŸ”§ [build_prompt_blocks] è®°å¿†IDå°†é€šè¿‡save_chat_conversationå‡½æ•°ä¿å­˜:")
        print(f"  - selected_ids: {selected_ids}")
        print(f"  - conversation_id: {conversation_id}")
        print(f"  - username: {username}")

        return "\n".join(parts)

    def _save_used_memories_to_conversation(
        self, conversation_id: str, memory_ids: List[str], username: str
    ) -> None:
        """ä¿å­˜å¯¹è¯ä¸­ä½¿ç”¨çš„å…±äº«è®°å¿†IDå’Œfocus_query"""
        import json
        import os

        try:
            print("\nğŸ”§ [pipeline_retrieve] å¼€å§‹ä¿å­˜ä½¿ç”¨çš„è®°å¿†ID:")
            print(f"  - å¯¹è¯ID: {conversation_id}")
            print(f"  - ç”¨æˆ·å: {username}")
            print(f"  - è®°å¿†IDåˆ—è¡¨: {memory_ids}")

            # æ„å»ºå¯¹è¯æ–‡ä»¶è·¯å¾„
            # ä»å½“å‰æ–‡ä»¶è·¯å¾„: /root/autodl-tmp/service/agent-share/sharememory_user/pipeline_retrieve.py
            # éœ€è¦åˆ°è¾¾: /root/autodl-tmp/service/agent-share/eval/memoryos_data
            # å½“å‰æ–‡ä»¶: __file__ = /root/autodl-tmp/service/agent-share/sharememory_user/pipeline_retrieve.py
            # ä¸Šä¸€çº§: os.path.dirname(__file__) = /root/autodl-tmp/service/agent-share/sharememory_user
            # ä¸Šä¸¤çº§: os.path.dirname(os.path.dirname(__file__)) = /root/autodl-tmp/service/agent-share
            # ç›®æ ‡: /root/autodl-tmp/service/agent-share/eval/memoryos_data
            MEMORYOS_DATA_DIR = os.path.join(
                os.path.dirname(os.path.dirname(__file__)), "eval", "memoryos_data"
            )
            conversation_file = os.path.join(
                MEMORYOS_DATA_DIR,
                "default_project",
                "users",
                username,
                f"{conversation_id}.json",
            )
            print(f"  - å¯¹è¯æ–‡ä»¶è·¯å¾„: {conversation_file}")
            print(f"  - å¯¹è¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(conversation_file)}")

            if os.path.exists(conversation_file):
                with open(conversation_file, "r", encoding="utf-8") as f:
                    conversation_data = json.load(f)

                # æ·»åŠ ä½¿ç”¨çš„è®°å¿†IDåˆ°å¯¹è¯æ•°æ®ä¸­
                if "used_memories" not in conversation_data:
                    conversation_data["used_memories"] = []

                # è·å–æ‰€æœ‰è®°å¿†ï¼Œç”¨äºæŸ¥æ‰¾focus_query
                all_memories = self.store.list_memories()
                memory_id_to_focus_query = {}
                for memory in all_memories:
                    memory_id_to_focus_query[memory.id] = memory.focus_query

                # å°†æ–°çš„è®°å¿†IDå’Œfocus_queryæ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼ˆé¿å…é‡å¤ï¼‰
                existing_memory_ids = set()
                for existing_memory in conversation_data["used_memories"]:
                    if isinstance(existing_memory, dict):
                        existing_memory_ids.add(existing_memory.get("id"))
                    else:
                        existing_memory_ids.add(existing_memory)

                for memory_id in memory_ids:
                    if memory_id not in existing_memory_ids:
                        focus_query = memory_id_to_focus_query.get(memory_id, "")
                        memory_info = {"id": memory_id, "focus_query": focus_query}
                        conversation_data["used_memories"].append(memory_info)
                        print(
                            f"âœ… [pipeline_retrieve] ä¿å­˜è®°å¿†ID: {memory_id}, focus_query: {focus_query[:50]}..."
                        )
                    else:
                        print(f"âš ï¸ [pipeline_retrieve] è®°å¿†IDå·²å­˜åœ¨ï¼Œè·³è¿‡: {memory_id}")

                # ä¿å­˜æ›´æ–°åçš„å¯¹è¯æ•°æ®
                with open(conversation_file, "w", encoding="utf-8") as f:
                    json.dump(conversation_data, f, ensure_ascii=False, indent=2)

                print(
                    f"âœ… [pipeline_retrieve] å·²ä¿å­˜ä½¿ç”¨çš„è®°å¿†IDå’Œfocus_queryåˆ°å¯¹è¯: {conversation_id}"
                )
            else:
                print(f"âš ï¸ [pipeline_retrieve] å¯¹è¯æ–‡ä»¶ä¸å­˜åœ¨: {conversation_file}")
        except Exception as e:
            print(f"âš ï¸ [pipeline_retrieve] ä¿å­˜ä½¿ç”¨çš„è®°å¿†IDå¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
