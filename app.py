# -*- coding: utf-8 -*-
import json
import os
import sys
import time
from datetime import datetime
from typing import Any, Dict, List, Optional

# è®¾ç½® Hugging Face é•œåƒæºï¼ˆè§£å†³è¿æ¥è¶…æ—¶é—®é¢˜ï¼‰
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

from flask import (
    Flask,
    Response,
    jsonify,
    render_template,
    request,
    send_from_directory,
)
from flask_cors import CORS

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# å…³é”®ï¼šå…ˆæ·»åŠ  sharememory_userï¼Œå†æ·»åŠ  memoryos-pypi
# è¿™æ · sharememory_user çš„æ¨¡å—ï¼ˆå¦‚ prompts, configï¼‰ä¼˜å…ˆçº§æ›´é«˜
sharememory_user_path = os.path.join(project_root, "sharememory_user")
sys.path.insert(0, sharememory_user_path)

memoryos_path = os.path.join(project_root, "memoryos-pypi")
sys.path.insert(0, memoryos_path)

# å¯¼å…¥MemoryOSç”¨äºä¸ªäººè®°å¿†
from memoryos import Memoryos

# å¯¼å…¥è¯„ä¼°æç¤ºè¯
from eval.evaluation_prompts import (
    get_baseline_answer_prompt,
    get_fusion_rag_prompt,
    get_rag_answer_prompt,
)
from sharememory_user.config import Config
from sharememory_user.models import UserProfile
from sharememory_user.pipeline_retrieve import RetrievePipeline
from sharememory_user.storage import JsonStore

app = Flask(__name__)
CORS(app)

# å…¨å±€å˜é‡
config = Config()
print(f"\n{'=' * 60}")
print("ğŸ“‚ é…ç½®ä¿¡æ¯:")
print(f"  - data_dir: {config.data_dir}")
print(f"  - memory_path: {config.memory_path}")
print(f"  - users_path: {config.users_path}")
print(f"{'=' * 60}\n")

store = JsonStore(config)
retrieve_pipeline = RetrievePipeline(config)
memoryos_instances = {}  # å­˜å‚¨æ¯ä¸ªç”¨æˆ·çš„MemoryOSå®ä¾‹

# å»¶è¿Ÿå¯¼å…¥ IngestPipeline
ingest_pipeline = None


def get_ingest_pipeline():
    """å»¶è¿Ÿåˆå§‹åŒ–å…±äº«è®°å¿†å­˜å‚¨ç®¡é“"""
    global ingest_pipeline
    if ingest_pipeline is None:
        # æ¸…é™¤ prompts æ¨¡å—ç¼“å­˜ï¼Œé¿å… memoryos çš„ prompts å¹²æ‰°
        if "prompts" in sys.modules:
            del sys.modules["prompts"]

        # å¯¼å…¥æ—¶ç¡®ä¿ä½œä¸ºåŒ…å¯¼å…¥
        from sharememory_user.pipeline_ingest import (
            IngestPipeline as SharedIngestPipeline,
        )

        ingest_pipeline = SharedIngestPipeline(config)
    return ingest_pipeline


# å¯¼å…¥ MemoryOS çš„å·¥å…·å‡½æ•°ç”¨äºæ£€æµ‹æ€ç»´é“¾æ–­è£‚
from memoryos import OpenAIClient

sys.path.insert(0, memoryos_path)
from utils import check_conversation_continuity

# ç”¨æˆ·æ•°æ®å­˜å‚¨ç›®å½• - ä½¿ç”¨memoryos_dataç»“æ„ï¼ŒæŒ‰ç…§é¡¹ç›®/ç”¨æˆ·å±‚çº§ç»„ç»‡
MEMORYOS_DATA_DIR = os.path.join(project_root, "eval", "memoryos_data")
os.makedirs(MEMORYOS_DATA_DIR, exist_ok=True)

# å…¨å±€ï¼šç»´æŠ¤æ¯ä¸ªç”¨æˆ·çš„å½“å‰å¯¹è¯é“¾ï¼ˆç”¨äºæ£€æµ‹æ€ç»´é“¾æ–­è£‚ï¼‰
user_conversation_chains = {}  # {user_id: [list of conversation pages]}


def save_chain_to_shared_memory(user_id: str, chain_pages: List[Dict]):
    """å°†å¯¹è¯é“¾ä¿å­˜åˆ°å…±äº«è®°å¿†"""
    if not chain_pages or len(chain_pages) < 1:
        return

    try:
        # è·å–ingest pipelineå®ä¾‹
        pipeline = get_ingest_pipeline()

        # ç¡®ä¿ç”¨æˆ·å­˜åœ¨äºå…±äº«è®°å¿†ç³»ç»Ÿä¸­
        user_profile = store.get_user(user_id)
        if not user_profile:
            user_config = get_user_config(user_id, "default_project")
            profile_text = user_config.get("user_profile", f"ç”¨æˆ· {user_id}")
            pipeline.ensure_user(user_id, profile_text)

        # å°†å¯¹è¯é“¾è½¬æ¢ä¸ºæ–‡æœ¬
        conversation_text = ""
        for page in chain_pages:
            user_msg = page.get("user_input", "")
            agent_msg = page.get("agent_response", "")
            timestamp = page.get("timestamp", "")
            conversation_text += f"User ({timestamp}): {user_msg}\n"
            conversation_text += f"Assistant ({timestamp}): {agent_msg}\n\n"

        # å­˜å‚¨åˆ°å…±äº«è®°å¿†
        memory_item = pipeline.ingest_dialog(user_id, conversation_text.strip())
        if memory_item:
            print(
                f"âœ… æˆåŠŸå°†æ€ç»´é“¾å­˜å‚¨åˆ°å…±äº«è®°å¿†ï¼ŒMemory ID: {memory_item.id}, å¯¹è¯è½®æ•°: {len(chain_pages)}"
            )
        else:
            print("âš ï¸ æ€ç»´é“¾æœªé€šè¿‡è´¨é‡æ£€æŸ¥ï¼Œæœªå­˜å‚¨åˆ°å…±äº«è®°å¿†")
    except Exception as e:
        print(f"âŒ å­˜å‚¨æ€ç»´é“¾åˆ°å…±äº«è®°å¿†å¤±è´¥: {e}")


def check_and_store_chain_break(
    user_id: str, new_user_msg: str, new_agent_msg: str
) -> None:
    """æ£€æµ‹æ€ç»´é“¾æ–­è£‚å¹¶å­˜å‚¨"""
    if user_id not in user_conversation_chains:
        user_conversation_chains[user_id] = []

    # åˆ›å»ºå½“å‰å¯¹è¯é¡µé¢
    current_page = {
        "user_input": new_user_msg,
        "agent_response": new_agent_msg,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    }

    chain = user_conversation_chains[user_id]

    # å¦‚æœæœ‰å†å²å¯¹è¯ï¼Œæ£€æµ‹æ˜¯å¦è¿ç»­
    if chain:
        last_page = chain[-1]

        # è·å– OpenAI å®¢æˆ·ç«¯ç”¨äºè¿ç»­æ€§æ£€æµ‹
        if user_id in memoryos_instances:
            memoryos_client = memoryos_instances[user_id].client
        else:
            # ä½¿ç”¨å…¨å±€é…ç½®åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯
            memoryos_client = OpenAIClient(
                api_key=config.openai_api_key, base_url=config.openai_api_base
            )

        # æ£€æµ‹å¯¹è¯è¿ç»­æ€§
        is_continuous = check_conversation_continuity(
            last_page, current_page, memoryos_client, model=config.llm_model_name
        )

        if not is_continuous:
            # æ€ç»´é“¾æ–­è£‚ï¼ä¿å­˜ä¹‹å‰çš„å¯¹è¯é“¾
            print(f"ğŸ’¡ æ£€æµ‹åˆ°ç”¨æˆ· {user_id} çš„æ€ç»´é“¾æ–­è£‚ï¼å½“å‰é“¾é•¿åº¦: {len(chain)}")
            save_chain_to_shared_memory(user_id, chain)
            # æ¸…ç©ºï¼Œå¼€å§‹æ–°çš„æ€ç»´é“¾
            user_conversation_chains[user_id] = [current_page]
        else:
            # å¯¹è¯è¿ç»­ï¼Œæ·»åŠ åˆ°å½“å‰é“¾
            chain.append(current_page)
    else:
        # ç¬¬ä¸€æ¡å¯¹è¯ï¼Œç›´æ¥æ·»åŠ 
        user_conversation_chains[user_id].append(current_page)


def ensure_user_memoryos(
    user_id: str, project_name: str = "default_project"
) -> Optional[Memoryos]:
    """ç¡®ä¿ç”¨æˆ·æœ‰MemoryOSå®ä¾‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»º"""
    project_name = user_id
    if user_id not in memoryos_instances:
        try:
            # åˆ›å»ºç”¨æˆ·æ•°æ®ç›®å½• - æŒ‰ç…§é¡¹ç›®/ç”¨æˆ·å±‚çº§ç»“æ„: eval/memoryos_data/{project}/users/{user_id}
            project_dir = os.path.join(MEMORYOS_DATA_DIR, project_name)
            user_data_dir = project_dir
            os.makedirs(user_data_dir, exist_ok=True)

            print(f"ğŸ“ åˆ›å»ºMemoryOSæ•°æ®ç›®å½•: {user_data_dir}")

            # åˆå§‹åŒ–MemoryOSå®ä¾‹
            memoryos_instance = Memoryos(
                user_id=user_id,
                openai_api_key=config.openai_api_key,
                data_storage_path=user_data_dir,
                openai_base_url=config.openai_api_base,
                llm_model=config.llm_model_name,
                assistant_id="chat_assistant",
                short_term_capacity=3,
                mid_term_capacity=2000,
                long_term_knowledge_capacity=100,
                retrieval_queue_capacity=3,
                mid_term_heat_threshold=8,
                mid_term_similarity_threshold=0.7,
                embedding_model_name="all-MiniLM-L6-v2",
            )

            memoryos_instances[user_id] = memoryos_instance
            # print(f"  - çŸ­æœŸè®°å¿†å®¹é‡: 3")
            # print(f"  - æ•°æ®å­˜å‚¨è·¯å¾„: {user_data_dir}")
        except Exception as e:
            print(f"åˆ›å»ºMemoryOSå®ä¾‹å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return None
    else:
        print(f"ç”¨æˆ· {user_id} çš„MemoryOSå®ä¾‹å·²å­˜åœ¨ï¼Œå¤ç”¨ç°æœ‰å®ä¾‹")

    return memoryos_instances.get(user_id)


def get_user_config(
    user_id: str, project_name: str = "default_project"
) -> Dict[str, Any]:
    """è·å–ç”¨æˆ·é…ç½®"""
    config_path = os.path.join(
        MEMORYOS_DATA_DIR, project_name, "users", user_id, "config.json"
    )
    if os.path.exists(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_user_config(
    user_id: str, config_data: Dict[str, Any], project_name: str = "default_project"
) -> bool:
    """ä¿å­˜ç”¨æˆ·é…ç½®"""
    try:
        user_dir = os.path.join(MEMORYOS_DATA_DIR, project_name, "users", user_id)
        os.makedirs(user_dir, exist_ok=True)

        config_path = os.path.join(user_dir, "config.json")
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(config_data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"ä¿å­˜ç”¨æˆ·é…ç½®å¤±è´¥: {e}")
        return False


def get_chat_conversations(
    user_id: str, project_name: str = "default_project"
) -> List[Dict[str, Any]]:
    """è·å–ç”¨æˆ·çš„èŠå¤©å¯¹è¯åˆ—è¡¨"""
    conversations_path = os.path.join(
        MEMORYOS_DATA_DIR, project_name, "users", user_id, "conversations.json"
    )
    if os.path.exists(conversations_path):
        with open(conversations_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []


def save_chat_conversations(
    user_id: str,
    conversations: List[Dict[str, Any]],
    project_name: str = "default_project",
) -> bool:
    """ä¿å­˜ç”¨æˆ·çš„èŠå¤©å¯¹è¯åˆ—è¡¨"""
    try:
        user_dir = os.path.join(MEMORYOS_DATA_DIR, project_name, "users", user_id)
        os.makedirs(user_dir, exist_ok=True)

        conversations_path = os.path.join(user_dir, "conversations.json")
        with open(conversations_path, "w", encoding="utf-8") as f:
            json.dump(conversations, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"ä¿å­˜èŠå¤©å¯¹è¯å¤±è´¥: {e}")
        return False


def get_chat_messages(
    user_id: str, conversation_id: str, project_name: str = "default_project"
) -> Optional[Dict[str, Any]]:
    """è·å–æŒ‡å®šå¯¹è¯çš„æ¶ˆæ¯"""
    conversation_path = os.path.join(
        MEMORYOS_DATA_DIR,
        project_name,
        "users",
        user_id,
        "conversations",
        f"{conversation_id}.json",
    )
    if os.path.exists(conversation_path):
        with open(conversation_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return None


def save_chat_conversation(
    username,
    conversation_id,
    user_message,
    ai_response,
    model,
    shared_memory_enabled=False,
    personal_memory_enabled=True,
    update_last_ai_message=False,
    user_message_only=False,
):
    """ä¿å­˜èŠå¤©å¯¹è¯åˆ°chatæ–‡ä»¶å¤¹"""

    # åˆ›å»ºç”¨æˆ·ç›®å½•
    user_chat_dir = os.path.join(
        MEMORYOS_DATA_DIR, "default_project", "users", username
    )
    os.makedirs(user_chat_dir, exist_ok=True)

    # å¦‚æœæ²¡æœ‰conversation_idï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
    if not conversation_id:
        conversation_id = f"chat_{int(time.time() * 1000)}"

    # å¯¹è¯æ–‡ä»¶è·¯å¾„
    conversation_file = os.path.join(user_chat_dir, f"{conversation_id}.json")

    # è¯»å–ç°æœ‰å¯¹è¯æˆ–åˆ›å»ºæ–°å¯¹è¯
    conversation_data = {
        "id": conversation_id,
        "username": username,
        "model": model,
        "shared_memory_enabled": shared_memory_enabled,
        "personal_memory_enabled": personal_memory_enabled,
        "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "messages": [],
    }

    if os.path.exists(conversation_file):
        try:
            with open(conversation_file, "r", encoding="utf-8") as f:
                conversation_data = json.load(f)
        except Exception as e:
            print(f"è¯»å–å¯¹è¯æ–‡ä»¶å¤±è´¥: {e}")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æœ€åä¸€æ¡AIæ¶ˆæ¯
    if update_last_ai_message and conversation_data.get("messages"):
        # æ›´æ–°æœ€åä¸€æ¡AIæ¶ˆæ¯
        messages = conversation_data["messages"]
        ai_message_found = False
        for i in range(len(messages) - 1, -1, -1):
            if messages[i]["type"] == "assistant":
                messages[i]["content"] = ai_response
                messages[i]["timestamp"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                ai_message_found = True
                break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°AIæ¶ˆæ¯ï¼Œåˆ™æ·»åŠ ä¸€ä¸ªï¼ˆè¿™ç§æƒ…å†µå‘ç”Ÿåœ¨åªä¿å­˜äº†ç”¨æˆ·æ¶ˆæ¯åï¼‰
        if not ai_message_found:
            new_ai_message = {
                "type": "assistant",
                "content": ai_response,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }
            conversation_data["messages"].append(new_ai_message)
    elif user_message_only:
        # åªæ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼ˆAIå›å¤ç¨åæ·»åŠ ï¼‰
        new_user_message = {
            "type": "user",
            "content": user_message,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        }
        conversation_data["messages"].append(new_user_message)
    else:
        # æ·»åŠ æ–°æ¶ˆæ¯å¯¹
        new_messages = [
            {
                "type": "user",
                "content": user_message,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            },
            {
                "type": "assistant",
                "content": ai_response,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            },
        ]

        conversation_data["messages"].extend(new_messages)

    conversation_data["updated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    conversation_data["shared_memory_enabled"] = shared_memory_enabled
    conversation_data["personal_memory_enabled"] = personal_memory_enabled

    # ç”Ÿæˆå¯¹è¯æ ‡é¢˜ï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
    if not conversation_data.get("title"):
        # ä½¿ç”¨ç”¨æˆ·çš„ç¬¬ä¸€æ¡æ¶ˆæ¯ä½œä¸ºæ ‡é¢˜
        first_user_message = next(
            (
                msg["content"]
                for msg in conversation_data["messages"]
                if msg["type"] == "user"
            ),
            "æ–°å¯¹è¯",
        )
        conversation_data["title"] = first_user_message[:30] + (
            "..." if len(first_user_message) > 30 else ""
        )

    # ä¿å­˜å¯¹è¯
    try:
        with open(conversation_file, "w", encoding="utf-8") as f:
            json.dump(conversation_data, f, ensure_ascii=False, indent=2)
        # print(f"âœ… å¯¹è¯å·²ä¿å­˜: {conversation_file}")
        return conversation_id
    except Exception as e:
        print(f"âŒ ä¿å­˜å¯¹è¯å¤±è´¥: {e}")
        return None


def load_conversation_history(username, conversation_id):
    """åŠ è½½å¯¹è¯å†å²"""
    try:
        conversation_file = os.path.join(
            MEMORYOS_DATA_DIR,
            "default_project",
            "users",
            username,
            f"{conversation_id}.json",
        )

        if os.path.exists(conversation_file):
            with open(conversation_file, "r", encoding="utf-8") as f:
                conversation_data = json.load(f)
            print(
                f"âœ… åŠ è½½å¯¹è¯å†å²æˆåŠŸï¼Œæ¶ˆæ¯æ•°: {len(conversation_data.get('messages', []))}"
            )
            return conversation_data
        else:
            # æ–°å¯¹è¯æ—¶æ–‡ä»¶ä¸å­˜åœ¨æ˜¯æ­£å¸¸çš„ï¼Œä¸éœ€è¦æ‰“å°é”™è¯¯
            print(f"â„¹ï¸ æ–°å¯¹è¯ï¼Œå°šæ— å†å²è®°å½•ï¼ˆå¯¹è¯ID: {conversation_id}ï¼‰")
            return None
    except Exception as e:
        print(f"âŒ åŠ è½½å¯¹è¯å†å²å¤±è´¥: {e}")
        return None


def get_chat_conversations(username, project_name="default_project"):
    """è·å–ç”¨æˆ·çš„å¯¹è¯åˆ—è¡¨"""
    try:
        user_dir = os.path.join(MEMORYOS_DATA_DIR, project_name, "users", username)
        if not os.path.exists(user_dir):
            return []

        conversations = []
        for filename in os.listdir(user_dir):
            if filename.endswith(".json"):
                conversation_id = filename[:-5]  # å»æ‰.jsonåç¼€
                conversation_file = os.path.join(user_dir, filename)

                try:
                    with open(conversation_file, "r", encoding="utf-8") as f:
                        conversation_data = json.load(f)

                    conversations.append(
                        {
                            "id": conversation_id,
                            "title": conversation_data.get("title", "æ–°å¯¹è¯"),
                            "created_at": conversation_data.get("created_at", ""),
                            "updated_at": conversation_data.get("updated_at", ""),
                            "message_count": len(conversation_data.get("messages", [])),
                        }
                    )
                except Exception as e:
                    print(f"âŒ è¯»å–å¯¹è¯æ–‡ä»¶å¤±è´¥ {filename}: {e}")
                    continue

        # æŒ‰æ›´æ–°æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        conversations.sort(key=lambda x: x.get("updated_at", ""), reverse=True)
        return conversations

    except Exception as e:
        print(f"âŒ è·å–å¯¹è¯åˆ—è¡¨å¤±è´¥: {e}")
        return []


def format_memoryos_retrieval_result(memoryos_result):
    """æ ¼å¼åŒ–MemoryOSæ£€ç´¢ç»“æœï¼Œä¸evaluate_end_to_end.pyä¿æŒä¸€è‡´"""
    if not memoryos_result:
        return ""

    formatted_context = ""

    # çŸ­æœŸè®°å¿† (recent conversations)
    if "short_term_queue" in memoryos_result and memoryos_result["short_term_queue"]:
        formatted_context += "SHORT-TERM MEMORY (Recent Interactions):\n"
        for i, item in enumerate(memoryos_result["short_term_queue"], 1):
            user_input = item.get("user_input", "")
            agent_response = item.get("agent_response", "")
            timestamp = item.get("timestamp", "")
            formatted_context += f"{i}. [{timestamp}] User: {user_input}\n"
            formatted_context += f"   Agent: {agent_response}\n\n"

    # ä¸­æœŸè®°å¿† (processed conversations/pages)
    if "mid_term_pages" in memoryos_result and memoryos_result["mid_term_pages"]:
        formatted_context += "MID-TERM MEMORY (Processed Conversations):\n"
        for i, page in enumerate(memoryos_result["mid_term_pages"], 1):
            content = page.get("content", "")
            if content:
                formatted_context += f"{i}. {content}\n\n"

    # ç”¨æˆ·é•¿æœŸçŸ¥è¯†
    if "user_knowledge" in memoryos_result and memoryos_result["user_knowledge"]:
        formatted_context += "LONG-TERM KNOWLEDGE (Personal Insights):\n"
        for i, knowledge in enumerate(memoryos_result["user_knowledge"], 1):
            knowledge_text = knowledge.get("knowledge", "") or knowledge.get(
                "content", ""
            )
            if knowledge_text:
                formatted_context += f"{i}. {knowledge_text}\n\n"

    # åŠ©æ‰‹é•¿æœŸçŸ¥è¯†
    if (
        "assistant_knowledge" in memoryos_result
        and memoryos_result["assistant_knowledge"]
    ):
        formatted_context += "ASSISTANT KNOWLEDGE (Domain Expertise):\n"
        for i, knowledge in enumerate(memoryos_result["assistant_knowledge"], 1):
            knowledge_text = knowledge.get("knowledge", "") or knowledge.get(
                "content", ""
            )
            if knowledge_text:
                formatted_context += f"{i}. {knowledge_text}\n\n"

    return formatted_context.strip()


def get_fusion_rag_prompt(
    user_query: str,
    shared_memory_context: str,
    personal_memory_context: str,
    user_profile: str,
) -> str:
    """
    åˆ›å»ºèåˆRAGæç¤ºè¯ï¼Œç»“åˆå…±äº«è®°å¿†å’Œä¸ªäººè®°å¿†
    ä¸evaluate_end_to_end.pyä¸­çš„get_fusion_rag_promptä¿æŒä¸€è‡´
    """
    return f"""You are a helpful AI assistant. Your task is to answer the user's question based on the provided context from two memory sources.
The context is retrieved from both shared knowledge base and your personal memory of past conversations.
Synthesize the information from both sources to provide a comprehensive and accurate answer.
If the context is not relevant, ignore it and answer based on your own knowledge.

**USER PROFILE:**
---
{user_profile}
---

**CONTEXT FROM SHARED MEMORY:**
---
{shared_memory_context}
---

**CONTEXT FROM PERSONAL MEMORY:**
---
{personal_memory_context}
---

**USER'S QUESTION:**
---
{user_query}
---

Important: Tailor your answer to the user's profile, expertise level, and professional background.
Your response should be relevant and appropriate for this specific user.
Combine insights from both shared and personal memory to provide the most helpful response.

Your Answer:
"""


def get_rag_answer_prompt(
    user_query: str, retrieved_context: str, user_profile: str
) -> str:
    """
    åˆ›å»ºä»…ä½¿ç”¨å…±äº«è®°å¿†çš„RAGæç¤ºè¯
    """
    return f"""You are a helpful AI assistant. Your task is to answer the user's question based on the provided context.
The context is retrieved from a shared knowledge base of past conversations.
Synthesize the information from the context to provide a comprehensive and accurate answer.
If the context is not relevant, ignore it and answer based on your own knowledge.

**USER PROFILE:**
---
{user_profile}
---

**CONTEXT FROM SHARED MEMORY:**
---
{retrieved_context}
---

**USER'S QUESTION:**
---
{user_query}
---

Important: Tailor your answer to the user's profile, expertise level, and professional background.
Your response should be relevant and appropriate for this specific user.

Your Answer:
"""


def get_baseline_answer_prompt(user_query: str, user_profile: str) -> str:
    """
    åˆ›å»ºä¸ä½¿ç”¨ä»»ä½•è®°å¿†çš„åŸºçº¿æç¤ºè¯
    """
    return f"""You are a helpful AI assistant. Please answer the user's question based on your own knowledge.

**USER PROFILE:**
---
{user_profile}
---

**USER'S QUESTION:**
---
{user_query}
---

Important: Tailor your answer to the user's profile, expertise level, and professional background.
Your response should be relevant and appropriate for this specific user.

Your Answer:
"""


def get_baseline_answer_prompt_no_profile(
    user_query: str, conversation_context: str = ""
) -> str:
    """
    åˆ›å»ºä¸ä½¿ç”¨ä»»ä½•è®°å¿†å’Œä¸ªäººä¿¡æ¯çš„åŸºçº¿æç¤ºè¯
    åªåŒ…å«å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    """
    context_section = ""
    if conversation_context:
        context_section = f"""
**CURRENT CONVERSATION CONTEXT:**
---
{conversation_context}
---

"""

    return f"""You are a helpful AI assistant. Please answer the user's question based on your own knowledge.
{context_section}**USER'S QUESTION:**
---
{user_query}
---

Please provide a helpful and accurate answer based on your knowledge. Keep your response clear and informative.

IMPORTANT: If the user asks about previous conversation content, you CAN see and reference the conversation history provided above. You should acknowledge what was said previously and provide helpful responses based on that context.

Your Answer:
"""


def generate_response_without_memory(
    user_id: str,
    message: str,
    model: str,
    project_name: str = "default_project",
    conversation_id: str = None,
) -> str:
    """
    æ— è®°å¿†æ¨¡å¼ï¼šåªæä¾›å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œä¸æä¾›ä»»ä½•ä¸ªäººä¿¡æ¯ã€å†å²è®°å¿†æˆ–å…±äº«è®°å¿†
    """
    try:
        # è·å–ç”¨æˆ·é…ç½®ï¼ˆä»…ç”¨äºAPIè°ƒç”¨ï¼‰
        user_config = get_user_config(user_id, project_name)
        if not user_config.get("openai_api_key"):
            return "é”™è¯¯ï¼šè¯·å…ˆé…ç½®OpenAI API Key"

        # æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼ˆä»…å½“å‰å¯¹è¯ï¼‰
        conversation_context = ""
        if conversation_id:
            conversation_data = load_conversation_history(user_id, conversation_id)
            if conversation_data and conversation_data.get("messages"):
                # è·å–å†å²æ¶ˆæ¯ï¼ˆæ’é™¤å½“å‰æ¶ˆæ¯ï¼‰
                history_messages = (
                    conversation_data["messages"][:-2]
                    if len(conversation_data["messages"]) >= 2
                    else []
                )
                if history_messages:
                    conversation_context = "\n".join(
                        [
                            f"{'ç”¨æˆ·' if msg['type'] == 'user' else 'åŠ©æ‰‹'}: {msg['content']}"
                            for msg in history_messages[-10:]  # åªå–æœ€è¿‘10æ¡å†å²æ¶ˆæ¯
                        ]
                    )
                    print(f"ğŸ“š ä½¿ç”¨æœ€è¿‘ {len(history_messages)} æ¡å¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡")
                else:
                    print("â„¹ï¸ æ–°å¯¹è¯çš„ç¬¬ä¸€è½®äº¤äº’")

        # åˆ›å»ºæ— è®°å¿†æç¤ºè¯ï¼ˆä¸åŒ…å«ä»»ä½•ä¸ªäººä¿¡æ¯ï¼‰
        prompt = get_baseline_answer_prompt_no_profile(message, conversation_context)

        # è°ƒç”¨LLMç”Ÿæˆå›å¤
        from openai import OpenAI

        client = OpenAI(
            api_key=user_config.get("openai_api_key", config.openai_api_key),
            base_url=user_config.get("openai_base_url", config.openai_api_base),
            timeout=120.0,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°120ç§’ï¼Œå¤„ç†é•¿ä¸Šä¸‹æ–‡
            max_retries=2,
        )

        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=10000,
        )

        return response.choices[0].message.content or "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›å¤ã€‚"

    except Exception as e:
        print(f"æ— è®°å¿†æ¨¡å¼ç”Ÿæˆå›å¤å¤±è´¥: {e}")
        return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›å¤æ—¶å‡ºç°é”™è¯¯: {str(e)}"


def get_rag_answer_prompt_with_context(
    user_query: str,
    retrieved_context: str,
    user_profile: str,
    conversation_context: str = "",
) -> str:
    """
    åˆ›å»ºåŒ…å«å¯¹è¯ä¸Šä¸‹æ–‡çš„RAGæç¤ºè¯
    """
    context_section = ""
    if conversation_context:
        context_section = f"""
**CURRENT CONVERSATION CONTEXT:**
---
{conversation_context}
---

"""

    return f"""You are a helpful AI assistant. Your task is to answer the user's question based on the provided context.
The context is retrieved from a shared knowledge base of past conversations.
Synthesize the information from the context to provide a comprehensive and accurate answer.
If the context is not relevant, ignore it and answer based on your own knowledge.
{context_section}**USER PROFILE:**
---
{user_profile}
---

**CONTEXT FROM SHARED MEMORY:**
---
{retrieved_context}
---

**USER'S QUESTION:**
---
{user_query}
---

Important: Tailor your answer to the user's profile, expertise level, and professional background.
Your response should be relevant and appropriate for this specific user.

IMPORTANT: You can see and reference the conversation history provided above. Use it to provide contextually relevant responses.

Your Answer:
"""


def get_fusion_rag_prompt_with_context(
    user_query: str,
    shared_memory_context: str,
    personal_memory_context: str,
    user_profile: str,
    conversation_context: str = "",
) -> str:
    """
    åˆ›å»ºåŒ…å«å¯¹è¯ä¸Šä¸‹æ–‡çš„èåˆRAGæç¤ºè¯
    """
    context_section = ""
    if conversation_context:
        context_section = f"""
**CURRENT CONVERSATION CONTEXT:**
---
{conversation_context}
---

"""

    return f"""You are a helpful AI assistant. Your task is to answer the user's question based on the provided context from two memory sources.
The context is retrieved from both shared knowledge base and your personal memory of past conversations.
Synthesize the information from both sources to provide a comprehensive and accurate answer.
If the context is not relevant, ignore it and answer based on your own knowledge.
{context_section}**USER PROFILE:**
---
{user_profile}
---

**CONTEXT FROM SHARED MEMORY:**
---
{shared_memory_context}
---

**CONTEXT FROM PERSONAL MEMORY:**
---
{personal_memory_context}
---

**USER'S QUESTION:**
---
{user_query}
---

Important: Tailor your answer to the user's profile, expertise level, and professional background.
Use both shared knowledge and personal context to provide a response that is relevant and appropriate for this specific user.
If there are conflicts between shared and personal memory, prioritize the information that is most relevant to the user's current question.

IMPORTANT: You can see and reference the conversation history provided above. Use it to provide contextually relevant responses.

Your Answer:
"""


def generate_response_with_memory(
    user_id: str,
    message: str,
    model: str,
    shared_memory_enabled: bool = False,
    personal_memory_enabled: bool = True,
    project_name: str = "default_project",
    conversation_id: str = None,
) -> str:
    """ç»“åˆä¸ªäººè®°å¿†å’Œå…±äº«è®°å¿†ç”Ÿæˆå›å¤ï¼Œä¸evaluate_end_to_end.pyé€»è¾‘ä¿æŒä¸€è‡´"""
    try:
        # è·å–ç”¨æˆ·é…ç½®
        user_config = get_user_config(user_id, project_name)
        if not user_config.get("openai_api_key"):
            return "é”™è¯¯ï¼šè¯·å…ˆé…ç½®OpenAI API Key"

        # ç¡®ä¿ç”¨æˆ·å­˜åœ¨
        user_profile = store.get_user(user_id)
        if not user_profile:
            # åˆ›å»ºé»˜è®¤ç”¨æˆ·æ¡£æ¡ˆ
            user_profile = UserProfile(
                user_id=user_id,
                profile_text=user_config.get("user_profile", f"ç”¨æˆ· {user_id}"),
            )
            store.add_user(user_profile)

        # æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼ˆæ— è®ºæ˜¯å¦å¼€å¯è®°å¿†éƒ½è¦æä¾›ï¼‰
        conversation_context = ""
        if conversation_id:
            conversation_data = load_conversation_history(user_id, conversation_id)
            if conversation_data and conversation_data.get("messages"):
                # è·å–å†å²æ¶ˆæ¯ï¼ˆæ’é™¤å½“å‰æ¶ˆæ¯ï¼‰
                history_messages = (
                    conversation_data["messages"][:-2]
                    if len(conversation_data["messages"]) >= 2
                    else []
                )
                if history_messages:
                    conversation_context = "\n".join(
                        [
                            f"{'ç”¨æˆ·' if msg['type'] == 'user' else 'åŠ©æ‰‹'}: {msg['content']}"
                            for msg in history_messages[-10:]  # åªå–æœ€è¿‘10æ¡å†å²æ¶ˆæ¯
                        ]
                    )
                    print(f"ğŸ“š ä½¿ç”¨æœ€è¿‘ {len(history_messages)} æ¡å¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡")
                else:
                    print("â„¹ï¸ æ–°å¯¹è¯çš„ç¬¬ä¸€è½®äº¤äº’")

        # è·å–ä¸ªäººè®°å¿†å’Œå¢å¼ºç”¨æˆ·æ¡£æ¡ˆ
        personal_memory_context = ""
        enhanced_profile_text = user_profile.profile_text

        if personal_memory_enabled and user_id in memoryos_instances:
            try:
                memoryos_instance = memoryos_instances[user_id]
                # ä½¿ç”¨MemoryOSæ ‡å‡†æ£€ç´¢æ–¹æ³•è·å–æ‰€æœ‰è®°å¿†å±‚ (é™ä½é˜ˆå€¼ä»¥æé«˜æ£€ç´¢æˆåŠŸç‡)
                memoryos_result = memoryos_instance.retriever.retrieve_context(
                    user_query=message,
                    user_id=user_id,
                    segment_similarity_threshold=0.1,  # é™ä½ä¸­æœŸè®°å¿†ç›¸ä¼¼åº¦é˜ˆå€¼
                    page_similarity_threshold=0.1,  # é™ä½é¡µé¢ç›¸ä¼¼åº¦é˜ˆå€¼
                    knowledge_threshold=0.1,  # é™ä½çŸ¥è¯†ç›¸ä¼¼åº¦é˜ˆå€¼
                    top_k_sessions=3,  # å‡å°‘ä¼šè¯æ•°é‡
                    top_k_knowledge=2,  # å¢åŠ çŸ¥è¯†æ•°é‡
                )

                # è·å–é•¿æœŸç”¨æˆ·æ¡£æ¡ˆ
                long_term_profile = (
                    memoryos_instance.user_long_term_memory.get_raw_user_profile(
                        user_id
                    )
                )
                if long_term_profile and long_term_profile != "None":
                    enhanced_profile_text = f"{user_profile.profile_text}\n\n**Long-term User Profile Insights (from MemoryOS):**\n{long_term_profile}"

                # æ·»åŠ çŸ­æœŸè®°å¿†åˆ°æ£€ç´¢ç»“æœä¸­
                context_result = memoryos_result.copy()
                # è·å–çŸ­æœŸè®°å¿†
                short_term_history = memoryos_instance.short_term_memory.get_all()
                if short_term_history:
                    context_result["short_term_queue"] = short_term_history

                # æ ¼å¼åŒ–ä¸ªäººè®°å¿†ä¸Šä¸‹æ–‡ï¼ˆæ’é™¤user_knowledgeä»¥é¿å…ä¸æ¡£æ¡ˆé‡å¤ï¼‰
                context_result.pop("user_knowledge", None)
                personal_memory_context = format_memoryos_retrieval_result(
                    context_result
                )

                print(
                    f"ğŸ§  Retrieved and formatted personal memory for {user_id}: {len(personal_memory_context)} chars"
                )

            except Exception as e:
                print(
                    f"âš ï¸ Failed to retrieve or process personal memory for {user_id}: {e}"
                )
                personal_memory_context = ""
        elif not personal_memory_enabled:
            print(f"ğŸš« Personal memory disabled for {user_id}, using baseline mode")

        # è·å–å…±äº«è®°å¿†
        shared_memory_context = ""
        if shared_memory_enabled:
            try:
                # ä½¿ç”¨ç¼“å­˜çš„peers (ä¸åŸå§‹é¡¹ç›®ä¸€è‡´)
                peers = retrieve_pipeline.get_cached_peers()

                # åˆ›å»ºå¢å¼ºçš„ç”¨æˆ·æ¡£æ¡ˆå¯¹è±¡ç”¨äºæ£€ç´¢ç®¡é“
                enhanced_user_profile = UserProfile(
                    user_id=user_id, profile_text=enhanced_profile_text
                )

                # æ£€ç´¢å…±äº«è®°å¿†
                retrieval_result = retrieve_pipeline.retrieve(
                    user=enhanced_user_profile, task=message, peers=peers, top_k=3
                )

                if retrieval_result["items"]:
                    shared_memory_context = retrieve_pipeline.build_prompt_blocks(
                        retrieval_result["items"]
                    )

                print(
                    f"ğŸ”— Retrieved shared memory context: {len(shared_memory_context)} chars"
                )

            except Exception as e:
                print(f"æ£€ç´¢å…±äº«è®°å¿†å¤±è´¥: {e}")

        # æ ¹æ®è®°å¿†çŠ¶æ€é€‰æ‹©æç¤ºè¯ (ä¸åŸå§‹é¡¹ç›®é€»è¾‘ä¸€è‡´)
        if (
            personal_memory_enabled
            and shared_memory_enabled
            and shared_memory_context
            and personal_memory_context
        ):
            # ä½¿ç”¨èåˆRAGæç¤ºè¯ (ä¸ªäººè®°å¿† + å…±äº«è®°å¿†)
            prompt = get_fusion_rag_prompt_with_context(
                message,
                shared_memory_context,
                personal_memory_context,
                enhanced_profile_text,
                conversation_context,
            )
            print("ğŸ§  Using Fusion RAG prompt (Personal + Shared Memory)")
        elif personal_memory_enabled and personal_memory_context:
            # ä½¿ç”¨ä¸ªäººè®°å¿†RAGæç¤ºè¯ (ä»…ä¸ªäººè®°å¿†)
            prompt = get_fusion_rag_prompt_with_context(
                message,
                "",  # æ— å…±äº«è®°å¿†
                personal_memory_context,
                enhanced_profile_text,
                conversation_context,
            )
            print("ğŸ§  Using Personal Memory RAG prompt")
        elif shared_memory_enabled and shared_memory_context:
            # ä½¿ç”¨å…±äº«è®°å¿†RAGæç¤ºè¯ (ä»…å…±äº«è®°å¿†)
            prompt = get_rag_answer_prompt_with_context(
                message,
                shared_memory_context,
                enhanced_profile_text,
                conversation_context,
            )
            print("ğŸ”— Using Shared Memory RAG prompt")
        else:
            # ä½¿ç”¨åŸºçº¿æç¤ºè¯ (æ— è®°å¿†) - ä¸åŒ…å«ç”¨æˆ·æ¡£æ¡ˆä¿¡æ¯
            prompt = get_baseline_answer_prompt_no_profile(
                message, conversation_context
            )
            print("ğŸ“ Using Baseline prompt (No Memory, No Profile)")

        # å¯¹è¯ä¸Šä¸‹æ–‡å·²ç»åœ¨ç›¸åº”çš„æç¤ºè¯å‡½æ•°ä¸­å¤„ç†äº†ï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–æ·»åŠ 

        # è°ƒç”¨LLMç”Ÿæˆå›å¤
        from openai import OpenAI

        client = OpenAI(
            api_key=user_config.get("openai_api_key", config.openai_api_key),
            base_url=user_config.get("openai_base_url", config.openai_api_base),
            timeout=120.0,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°120ç§’ï¼Œå¤„ç†é•¿ä¸Šä¸‹æ–‡
            max_retries=2,
        )

        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=10000,
        )

        return response.choices[0].message.content or "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"

    except Exception as e:
        print(f"ç”Ÿæˆå›å¤å¤±è´¥: {e}")
        return f"é”™è¯¯ï¼šç”Ÿæˆå›å¤æ—¶å‡ºç°é—®é¢˜ - {str(e)}"


# APIè·¯ç”±
@app.route("/")
def index():
    """ä¸»é¡µé¢"""
    return render_template("index.html")


@app.route("/dashboard")
def dashboard():
    """è®°å¿†ä»ªè¡¨ç›˜é¡µé¢"""
    return render_template("dashboard.html")


@app.route("/api/get_shared_memories", methods=["POST"])
def get_shared_memories():
    """è·å–å…±äº«è®°å¿†API"""
    try:
        data = request.get_json()
        username = data.get("username")
        limit = data.get("limit", 50)

        print("\nğŸ“Š è·å–å…±äº«è®°å¿†è¯·æ±‚:")
        print(f"  - ç”¨æˆ·å: {username}")
        print(f"  - é™åˆ¶æ•°é‡: {limit}")

        if not username:
            return jsonify({"success": False, "error": "ç¼ºå°‘ç”¨æˆ·å"})

        # è·å–æ‰€æœ‰å…±äº«è®°å¿†
        all_memories = store.list_memories()
        print(f"  - æ€»è®°å¿†æ•°é‡: {len(all_memories)}")

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        memories_list = []
        for i, mem in enumerate(all_memories[:limit]):
            try:
                # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºå¯è¯»æ ¼å¼
                timestamp_str = (
                    datetime.fromtimestamp(mem.created_at).strftime("%Y-%m-%d %H:%M:%S")
                    if mem.created_at
                    else "æœªçŸ¥æ—¶é—´"
                )

                # å®‰å…¨åœ°è·å–å†…å®¹
                content = ""
                if hasattr(mem, "cot_text") and mem.cot_text and mem.cot_text.strip():
                    content = mem.cot_text.strip()
                elif hasattr(mem, "raw_text") and mem.raw_text and mem.raw_text.strip():
                    content = mem.raw_text.strip()
                else:
                    content = "æ— å†…å®¹"

                memory_data = {
                    "id": mem.id,
                    "user_id": mem.source_user_id,
                    "content": content,
                    "timestamp": timestamp_str,
                    "source": mem.meta.get("source", "conversation")
                    if hasattr(mem, "meta") and mem.meta
                    else "conversation",
                }
                memories_list.append(memory_data)

                if i < 3:  # æ‰“å°å‰3æ¡è®°å¿†çš„è¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•
                    print(
                        f"  - è®°å¿† {i + 1}: ID={mem.id}, ç”¨æˆ·={mem.source_user_id}, å†…å®¹é•¿åº¦={len(memory_data['content'])}"
                    )

            except Exception as mem_error:
                print(f"  - å¤„ç†è®°å¿† {i} å¤±è´¥: {mem_error}")
                continue

        print(f"  - æˆåŠŸå¤„ç†è®°å¿†æ•°é‡: {len(memories_list)}")

        return jsonify(
            {"success": True, "memories": memories_list, "total": len(all_memories)}
        )

    except Exception as e:
        print(f"âŒ è·å–å…±äº«è®°å¿†å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return jsonify({"success": False, "error": str(e)})


@app.route("/api/get_memory_file", methods=["GET"])
def get_memory_file():
    """è·å–ç”¨æˆ·çš„è®°å¿†æ–‡ä»¶ï¼ˆçŸ­æœŸã€ä¸­æœŸã€é•¿æœŸï¼‰"""
    try:
        username = request.args.get("username")
        file_name = request.args.get("file")

        if not username or not file_name:
            return jsonify({"success": False, "error": "ç¼ºå°‘å‚æ•°"}), 400

        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åˆæ³•
        allowed_files = ["short_term.json", "mid_term.json", "long_term_user.json"]
        if file_name not in allowed_files:
            return jsonify({"success": False, "error": "éæ³•çš„æ–‡ä»¶å"}), 400

        # æ„å»ºæ–‡ä»¶è·¯å¾„ï¼ševal/memoryos_data/{username}/users/{username}/{file}
        file_path = os.path.join(
            MEMORYOS_DATA_DIR, username, "users", username, file_name
        )

        print(f"å°è¯•è¯»å–è®°å¿†æ–‡ä»¶: {file_path}")

        if not os.path.exists(file_path):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return jsonify({"success": False, "error": "æ–‡ä»¶ä¸å­˜åœ¨"}), 404

        # è¯»å–æ–‡ä»¶
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        return jsonify(data)

    except Exception as e:
        print(f"è¯»å–è®°å¿†æ–‡ä»¶å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/chat_direct", methods=["POST"])
def chat_direct():
    """æµå¼èŠå¤©API - ä½¿ç”¨Server-Sent Events"""
    from flask import stream_with_context

    # åœ¨æµå¼ä¸Šä¸‹æ–‡å¤–è¯»å–è¯·æ±‚æ•°æ®
    data = request.get_json()
    username = data.get("username")
    message = data.get("message")
    model = data.get("model", "gpt-4o-mini")
    conversation_id = data.get("conversation_id")
    shared_memory_enabled = data.get("shared_memory_enabled", False)
    personal_memory_enabled = data.get("personal_memory_enabled", True)
    project_name = data.get("project_name", "default_project")

    def generate():
        try:
            if not username or not message:
                yield f"data: {json.dumps({'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}, ensure_ascii=False)}\n\n"
                return

            # ç¡®ä¿ç”¨æˆ·æœ‰MemoryOSå®ä¾‹
            print(f"\n{'=' * 60}")
            print(f"ğŸ“ [æµå¼] å¼€å§‹å¤„ç†ç”¨æˆ· {username} çš„æ¶ˆæ¯")
            print(
                f"ğŸ”˜ ä¸ªäººè®°å¿†: {personal_memory_enabled}, å…±äº«è®°å¿†: {shared_memory_enabled}"
            )

            memoryos_instance = ensure_user_memoryos(username, project_name)
            if not memoryos_instance and username in memoryos_instances:
                del memoryos_instances[username]
                memoryos_instance = ensure_user_memoryos(username, project_name)

            # è·å–ç”¨æˆ·é…ç½®
            user_config = get_user_config(username, project_name)
            if not user_config.get("openai_api_key"):
                yield f"data: {json.dumps({'error': 'è¯·å…ˆé…ç½®OpenAI API Key'}, ensure_ascii=False)}\n\n"
                return

            # ä½¿ç”¨å±€éƒ¨å˜é‡æ¥é¿å…ä½œç”¨åŸŸé—®é¢˜
            current_conversation_id = conversation_id

            # æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡
            conversation_context = ""
            if current_conversation_id:
                conversation_data = load_conversation_history(
                    username, current_conversation_id
                )
                if conversation_data and conversation_data.get("messages"):
                    history_messages = conversation_data["messages"]
                    if history_messages:
                        conversation_context = "\n".join(
                            [
                                f"{'ç”¨æˆ·' if msg['type'] == 'user' else 'åŠ©æ‰‹'}: {msg['content']}"
                                for msg in history_messages[-10:]
                            ]
                        )

            # è·å–ç”¨æˆ·æ¡£æ¡ˆ
            user_profile = store.get_user(username)
            if not user_profile:
                user_profile = UserProfile(
                    user_id=username,
                    profile_text=user_config.get("user_profile", f"ç”¨æˆ· {username}"),
                )
                store.add_user(user_profile)

            # è·å–ä¸ªäººè®°å¿†å’Œå…±äº«è®°å¿†
            personal_memory_context = ""
            enhanced_profile_text = user_profile.profile_text

            if personal_memory_enabled and username in memoryos_instances:
                try:
                    memoryos_instance = memoryos_instances[username]
                    memoryos_result = memoryos_instance.retriever.retrieve_context(
                        user_query=message,
                        user_id=username,
                        segment_similarity_threshold=0.1,
                        page_similarity_threshold=0.1,
                        knowledge_threshold=0.1,
                        top_k_sessions=3,
                        top_k_knowledge=2,
                    )

                    long_term_profile = (
                        memoryos_instance.user_long_term_memory.get_raw_user_profile(
                            username
                        )
                    )
                    if long_term_profile and long_term_profile != "None":
                        enhanced_profile_text = f"{user_profile.profile_text}\n\n**Long-term User Profile Insights:**\n{long_term_profile}"

                    context_result = memoryos_result.copy()
                    short_term_history = memoryos_instance.short_term_memory.get_all()
                    if short_term_history:
                        context_result["short_term_queue"] = short_term_history

                    context_result.pop("user_knowledge", None)
                    personal_memory_context = format_memoryos_retrieval_result(
                        context_result
                    )
                except Exception as e:
                    print(f"âš ï¸ è·å–ä¸ªäººè®°å¿†å¤±è´¥: {e}")

            shared_memory_context = ""
            if shared_memory_enabled:
                try:
                    peers = retrieve_pipeline.get_cached_peers()
                    enhanced_user_profile = UserProfile(
                        user_id=username, profile_text=enhanced_profile_text
                    )
                    retrieval_result = retrieve_pipeline.retrieve(
                        user=enhanced_user_profile, task=message, peers=peers, top_k=3
                    )
                    if retrieval_result["items"]:
                        shared_memory_context = retrieve_pipeline.build_prompt_blocks(
                            retrieval_result["items"]
                        )
                except Exception as e:
                    print(f"âš ï¸ è·å–å…±äº«è®°å¿†å¤±è´¥: {e}")

            # æ„å»ºæç¤ºè¯
            if (
                personal_memory_enabled
                and shared_memory_enabled
                and shared_memory_context
                and personal_memory_context
            ):
                prompt = get_fusion_rag_prompt_with_context(
                    message,
                    shared_memory_context,
                    personal_memory_context,
                    enhanced_profile_text,
                    conversation_context,
                )
                print("ğŸ§  ä½¿ç”¨èåˆRAGæç¤ºè¯")
            elif personal_memory_enabled and personal_memory_context:
                prompt = get_fusion_rag_prompt_with_context(
                    message,
                    "",
                    personal_memory_context,
                    enhanced_profile_text,
                    conversation_context,
                )
                print("ğŸ§  ä½¿ç”¨ä¸ªäººè®°å¿†RAGæç¤ºè¯")
            elif shared_memory_enabled and shared_memory_context:
                prompt = get_rag_answer_prompt_with_context(
                    message,
                    shared_memory_context,
                    enhanced_profile_text,
                    conversation_context,
                )
                print("ğŸ”— ä½¿ç”¨å…±äº«è®°å¿†RAGæç¤ºè¯")
            else:
                prompt = get_baseline_answer_prompt_no_profile(
                    message, conversation_context
                )
                print("ğŸ“ ä½¿ç”¨åŸºçº¿æç¤ºè¯")

            # ğŸš€ ç«‹å³ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼Œç¡®ä¿å¯¹è¯æ–‡ä»¶å­˜åœ¨ï¼Œé¿å…åˆ‡æ¢æ—¶çš„"å¯¹è¯ä¸å­˜åœ¨"é”™è¯¯
            try:
                temp_conversation_id = save_chat_conversation(
                    username,
                    current_conversation_id,
                    message,
                    "",
                    model,
                    shared_memory_enabled,
                    personal_memory_enabled,
                    user_message_only=True,
                )
                print(f"âœ… å·²ç«‹å³ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼Œå¯¹è¯ID: {temp_conversation_id}")
                if temp_conversation_id:
                    current_conversation_id = temp_conversation_id
            except Exception as e:
                print(f"âš ï¸ ç«‹å³ä¿å­˜ç”¨æˆ·æ¶ˆæ¯å¤±è´¥: {e}")

            # ğŸ”¥ æµå¼è°ƒç”¨OpenAI API
            from openai import OpenAI

            client = OpenAI(
                api_key=user_config.get("openai_api_key", config.openai_api_key),
                base_url=user_config.get("openai_base_url", config.openai_api_base),
                timeout=120.0,
                max_retries=2,
            )

            # ğŸ”¥ åˆ›å»ºå¯ä¸­æ–­çš„æµå¼è°ƒç”¨
            try:
                stream = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    max_tokens=10000,
                    stream=True,  # ğŸ”¥ å¯ç”¨æµå¼è¾“å‡º
                )
            except Exception as e:
                yield f"data: {json.dumps({'error': f'åˆ›å»ºæµå¼è°ƒç”¨å¤±è´¥: {str(e)}'}, ensure_ascii=False)}\n\n"
                return

            # æ”¶é›†å®Œæ•´å›å¤
            full_response = ""
            stream_interrupted = False
            chunk_count = 0  # ç”¨äºå®šæœŸä¿å­˜

            # å®šä¹‰ä¸€ä¸ªä¿å­˜å‡½æ•°ï¼Œç”¨äºåœ¨ä»»ä½•æƒ…å†µä¸‹ä¿å­˜å¯¹è¯
            def save_interrupted_conversation():
                """ä¿å­˜è¢«ä¸­æ–­çš„å¯¹è¯"""
                # å³ä½¿æ²¡æœ‰AIå›å¤ï¼Œä¹Ÿè¦ä¿å­˜ç”¨æˆ·çš„æ¶ˆæ¯
                response_to_save = (
                    full_response if full_response.strip() else "ï¼ˆå›å¤è¢«ç”¨æˆ·ç»ˆæ­¢ï¼‰"
                )
                print(
                    f"ğŸ’¾ ä¿å­˜è¢«ä¸­æ–­çš„å¯¹è¯ï¼Œç”¨æˆ·æ¶ˆæ¯: {message[:50]}...ï¼ŒAIå›å¤é•¿åº¦: {len(full_response)} å­—ç¬¦"
                )

                try:
                    # ä¿å­˜åˆ°ä¸ªäººè®°å¿†ï¼ˆå³ä½¿AIå›å¤ä¸ºç©ºä¹Ÿè¦ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼‰
                    if username in memoryos_instances:
                        memoryos_instance = memoryos_instances[username]
                        memoryos_instance.add_memory(message, response_to_save)
                        print("âœ… ä¸­æ–­çš„å¯¹è¯å·²ä¿å­˜åˆ°çŸ­æœŸè®°å¿†")

                    # ä¿å­˜å¯¹è¯åˆ°æ–‡ä»¶ - æ›´æ–°æœ€åä¸€æ¡AIæ¶ˆæ¯
                    saved_conversation_id = save_chat_conversation(
                        username,
                        current_conversation_id,
                        message,
                        response_to_save,
                        model,
                        shared_memory_enabled,
                        personal_memory_enabled,
                        update_last_ai_message=True,
                    )

                    # æ£€æµ‹æ€ç»´é“¾æ–­è£‚ï¼ˆåªæœ‰AIæœ‰å›å¤æ—¶æ‰æ£€æµ‹ï¼‰
                    if shared_memory_enabled and full_response.strip():
                        try:
                            check_and_store_chain_break(
                                username, message, full_response
                            )
                        except Exception as e:
                            print(f"âš ï¸ æ€ç»´é“¾æ£€æµ‹å¤±è´¥: {e}")

                    print(
                        f"âœ… ä¸­æ–­çš„æ¶ˆæ¯å·²æ­£å¸¸ä¿å­˜ï¼Œconversation_id: {saved_conversation_id}"
                    )
                    return saved_conversation_id
                except Exception as e:
                    print(f"âŒ ä¿å­˜ä¸­æ–­æ¶ˆæ¯å¤±è´¥: {e}")
                    import traceback

                    traceback.print_exc()
                    return None

            try:
                # é€å—å‘é€æ•°æ®
                for chunk in stream:
                    try:
                        # æ£€æŸ¥ choices æ˜¯å¦ä¸ºç©º
                        if chunk.choices and len(chunk.choices) > 0:
                            delta = chunk.choices[0].delta
                            if delta.content:
                                content = delta.content
                                full_response += content
                                chunk_count += 1

                                # ğŸ”„ å®šæœŸä¿å­˜AIå›å¤å†…å®¹ï¼ˆæ¯10ä¸ªchunkä¿å­˜ä¸€æ¬¡ï¼‰ï¼Œç¡®ä¿åˆ‡æ¢æ—¶èƒ½çœ‹åˆ°å·²ç”Ÿæˆçš„å†…å®¹
                                if chunk_count % 10 == 0:
                                    try:
                                        save_chat_conversation(
                                            username,
                                            current_conversation_id,
                                            message,
                                            full_response,
                                            model,
                                            shared_memory_enabled,
                                            personal_memory_enabled,
                                            update_last_ai_message=True,
                                        )
                                        # print(f"ğŸ”„ å·²ä¿å­˜AIå›å¤ç‰‡æ®µï¼Œé•¿åº¦: {len(full_response)} å­—ç¬¦")
                                    except Exception as e:
                                        print(f"âš ï¸ ä¿å­˜AIå›å¤ç‰‡æ®µå¤±è´¥: {e}")

                                # å‘é€SSEæ ¼å¼æ•°æ®
                                yield f"data: {json.dumps({'content': content}, ensure_ascii=False)}\n\n"
                    except (GeneratorExit, StopIteration) as e:
                        # å®¢æˆ·ç«¯æ–­å¼€è¿æ¥
                        print(f"ğŸ›‘ æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥: {e}")
                        stream_interrupted = True
                        break
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†æµå¼æ•°æ®æ—¶å‡ºé”™: {e}")
                        continue

            except GeneratorExit:
                # å®¢æˆ·ç«¯ä¸»åŠ¨æ–­å¼€è¿æ¥ï¼ˆAbortControllerè§¦å‘ï¼‰
                print("ğŸ›‘ å®¢æˆ·ç«¯ä¸»åŠ¨ç»ˆæ­¢è¿æ¥ï¼ˆAbortControllerï¼‰ - å¼€å§‹ä¿å­˜æ“ä½œ")
                stream_interrupted = True

                # å°è¯•å…³é—­ OpenAI æµ
                try:
                    if hasattr(stream, "close"):
                        stream.close()
                    print("ğŸ”’ OpenAI æµå·²å…³é—­")
                except Exception as e:
                    print(f"âš ï¸ å…³é—­ OpenAI æµæ—¶å‡ºé”™: {e}")

                # ğŸš€ ç«‹å³ä¿å­˜è¢«ä¸­æ–­çš„å†…å®¹ï¼ˆåœ¨yieldä¹‹å‰ï¼‰
                try:
                    saved_conversation_id = save_interrupted_conversation()
                    print(f"ğŸ’¾ ä¿å­˜æ“ä½œå®Œæˆï¼Œç»“æœ: {saved_conversation_id}")
                except Exception as e:
                    print(f"âŒ ä¿å­˜æ“ä½œå‡ºç°å¼‚å¸¸: {e}")
                    saved_conversation_id = None

                # å°è¯•å‘é€ç»ˆæ­¢ä¿¡å·ï¼ˆå¦‚æœå¯èƒ½çš„è¯ï¼‰
                try:
                    yield f"data: {json.dumps({'done': True, 'conversation_id': saved_conversation_id}, ensure_ascii=False)}\n\n"
                    print("ğŸ“¤ ç»ˆæ­¢ä¿¡å·å‘é€æˆåŠŸ")
                except Exception as e:
                    # å¦‚æœæ— æ³•å‘é€ï¼Œä¹Ÿæ²¡å…³ç³»ï¼Œå› ä¸ºå®¢æˆ·ç«¯å·²ç»æ–­å¼€
                    print(f"âš ï¸ æ— æ³•å‘é€ç»ˆæ­¢ä¿¡å·: {e}")

                print("ğŸ›‘ GeneratorExit å¼‚å¸¸å¤„ç†å®Œæˆ")
                return

            except Exception as e:
                print(f"âŒ æµå¼å¤„ç†å‡ºç°å¼‚å¸¸: {e}")
                yield f"data: {json.dumps({'error': f'æµå¼å¤„ç†å¼‚å¸¸: {str(e)}'}, ensure_ascii=False)}\n\n"
                return

            # å¦‚æœè¢«ä¸­æ–­ï¼Œä¿å­˜ä¸­æ–­çš„å†…å®¹ç„¶åè¿”å›
            if stream_interrupted:
                print("ğŸ›‘ æµå¼è¾“å‡ºè¢«ä¸­æ–­ï¼Œå°è¯•ä¿å­˜å·²ç”Ÿæˆçš„å†…å®¹")
                saved_conversation_id = save_interrupted_conversation()
                try:
                    yield f"data: {json.dumps({'done': True, 'conversation_id': saved_conversation_id}, ensure_ascii=False)}\n\n"
                except:
                    pass
                return

            # å‘é€ç»“æŸæ ‡è®°
            yield f"data: {json.dumps({'done': True}, ensure_ascii=False)}\n\n"

            print(f"âœ… æµå¼è¾“å‡ºå®Œæˆï¼Œæ€»é•¿åº¦: {len(full_response)} å­—ç¬¦")

            # åªæœ‰åœ¨æ­£å¸¸å®Œæˆæ—¶æ‰ä¿å­˜è®°å¿†å’Œå¯¹è¯ï¼ˆä¸åŒ…å«è¢«ä¸­æ–­çš„æƒ…å†µï¼‰
            if full_response.strip():  # ç¡®ä¿æœ‰å†…å®¹æ‰ä¿å­˜
                # ä¿å­˜åˆ°ä¸ªäººè®°å¿†
                if username in memoryos_instances:
                    try:
                        memoryos_instance = memoryos_instances[username]
                        memoryos_instance.add_memory(message, full_response)
                        print("âœ… å¯¹è¯å·²ä¿å­˜åˆ°çŸ­æœŸè®°å¿†")
                    except Exception as e:
                        print(f"âŒ ä¿å­˜è®°å¿†å¤±è´¥: {e}")

                # ä¿å­˜å¯¹è¯ - æ›´æ–°æœ€åä¸€æ¡AIæ¶ˆæ¯
                saved_conversation_id = save_chat_conversation(
                    username,
                    current_conversation_id,
                    message,
                    full_response,
                    model,
                    shared_memory_enabled,
                    personal_memory_enabled,
                    update_last_ai_message=True,
                )

                # æ£€æµ‹æ€ç»´é“¾æ–­è£‚
                if shared_memory_enabled:
                    try:
                        check_and_store_chain_break(username, message, full_response)
                    except Exception as e:
                        print(f"âš ï¸ æ€ç»´é“¾æ£€æµ‹å¤±è´¥: {e}")

                # å‘é€å¯¹è¯ID
                yield f"data: {json.dumps({'conversation_id': saved_conversation_id or current_conversation_id}, ensure_ascii=False)}\n\n"
            else:
                print("âš ï¸ æµå¼è¾“å‡ºä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜æ“ä½œ")

        except Exception as e:
            print(f"âŒ æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            yield f"data: {json.dumps({'error': str(e)}, ensure_ascii=False)}\n\n"

    return Response(
        stream_with_context(generate()),
        mimetype="text/event-stream",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no"},
    )


@app.route("/get_chat_conversations", methods=["POST"])
def get_chat_conversations_api():
    """è·å–èŠå¤©å¯¹è¯åˆ—è¡¨"""
    try:
        data = request.get_json()
        username = data.get("username")
        project_name = data.get("project_name", "default_project")

        if not username:
            return jsonify({"success": False, "error": "ç¼ºå°‘ç”¨æˆ·å"})

        conversations = get_chat_conversations(username, project_name)
        return jsonify({"success": True, "conversations": conversations})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)})


@app.route("/get_chat_messages", methods=["POST"])
def get_chat_messages_api():
    """è·å–æŒ‡å®šå¯¹è¯çš„æ¶ˆæ¯"""
    try:
        data = request.get_json()
        username = data.get("username")
        conversation_id = data.get("conversation_id")
        project_name = data.get("project_name", "default_project")

        if not username or not conversation_id:
            return jsonify({"success": False, "error": "ç¼ºå°‘å¿…è¦å‚æ•°"})

        conversation = load_conversation_history(username, conversation_id)
        if not conversation:
            return jsonify({"success": False, "error": "å¯¹è¯ä¸å­˜åœ¨"})

        return jsonify({"success": True, "conversation": conversation})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)})


@app.route("/save_chat_user_config", methods=["POST"])
def save_chat_user_config():
    """ä¿å­˜ç”¨æˆ·é…ç½®"""
    try:
        data = request.get_json()
        username = data.get("username")
        project_name = data.get("project_name", "default_project")

        if not username:
            return jsonify({"success": False, "error": "ç¼ºå°‘ç”¨æˆ·å"})

        config_data = {
            "openai_api_key": data.get("openai_api_key", ""),
            "openai_base_url": data.get("openai_base_url", "https://api.openai.com/v1"),
            "user_profile": data.get("user_profile", f"ç”¨æˆ· {username}"),
        }

        if save_user_config(username, config_data, project_name):
            return jsonify({"success": True})
        else:
            return jsonify({"success": False, "error": "ä¿å­˜é…ç½®å¤±è´¥"})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)})


@app.route("/api/login", methods=["POST"])
def login():
    """ç”¨æˆ·ç™»å½•éªŒè¯API"""
    try:
        data = request.get_json()
        username = data.get("username")
        password = data.get("password")

        if not username or not password:
            return jsonify({"success": False, "error": "ç”¨æˆ·åå’Œå¯†ç ä¸èƒ½ä¸ºç©º"})

        # è¯»å–ç”¨æˆ·é…ç½®æ–‡ä»¶
        user_file_path = os.path.join(project_root, "user.json")
        if not os.path.exists(user_file_path):
            return jsonify({"success": False, "error": "ç”¨æˆ·é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"})

        with open(user_file_path, "r", encoding="utf-8") as f:
            user_data = json.load(f)

        # éªŒè¯ç”¨æˆ·åå’Œå¯†ç 
        users = user_data.get("users", [])
        for user in users:
            if user.get("username") == username and user.get("password") == password:
                return jsonify(
                    {"success": True, "message": "ç™»å½•æˆåŠŸ", "username": username}
                )

        return jsonify({"success": False, "error": "ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"})

    except Exception as e:
        print(f"ç™»å½•éªŒè¯å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return jsonify({"success": False, "error": str(e)})


@app.route("/chat/users/<username>/<filename>")
@app.route("/chat/<project_name>/users/<username>/<filename>")
def serve_user_file(username, filename, project_name="default_project"):
    """æä¾›ç”¨æˆ·æ–‡ä»¶æœåŠ¡"""
    user_dir = os.path.join(MEMORYOS_DATA_DIR, project_name, "users", username)
    if os.path.exists(os.path.join(user_dir, filename)):
        return send_from_directory(user_dir, filename)
    else:
        return jsonify({"error": "æ–‡ä»¶ä¸å­˜åœ¨"}), 404


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨Flaskåº”ç”¨...")
    print(f"ğŸ“ æ•°æ®ç›®å½•: {MEMORYOS_DATA_DIR}")
    print("ğŸŒ è®¿é—®åœ°å€: http://127.0.0.1:5002")
    app.run(host="127.0.0.1", port=5002, debug=True)
